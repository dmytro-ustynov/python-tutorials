# –ü—Ä–∞–∫—Ç–∏—á–Ω–µ –∑–∞–Ω—è—Ç—Ç—è 4-4: –û—Å–Ω–æ–≤–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É

## üìã –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∑–∞–Ω—è—Ç—Ç—è

**–¢–µ–º–∞:** –û—Å–Ω–æ–≤–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É  
**–ü—ñ–¥—Ç–µ–º–∏:**
1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤. CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∏
2. –ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É HTML

**–¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å:** 2 –≥–æ–¥–∏–Ω–∏ (120 —Ö–≤–∏–ª–∏–Ω)  
**–¢–∏–ø –∑–∞–Ω—è—Ç—Ç—è:** –ü—Ä–∞–∫—Ç–∏—á–Ω–µ  
**–ü–æ–ø–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞–Ω–Ω—è:** 
- –†–æ–±–æ—Ç–∞ –∑ API —á–µ—Ä–µ–∑ requests
- –û–±—Ä–æ–±–∫–∞ JSON/XML –¥–∞–Ω–∏—Ö
- –û—Å–Ω–æ–≤–∏ –û–û–ü –≤ Python

---

## üéØ –¶—ñ–ª—ñ –∑–∞–Ω—è—Ç—Ç—è

### –ù–∞–≤—á–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ:
1. –†–æ–∑—É–º—ñ—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É HTML –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ DOM
2. –í–æ–ª–æ–¥—ñ—Ç–∏ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏ –¥–ª—è –ø–æ—à—É–∫—É –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É HTML
4. –Ü–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏

### –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –Ω–∞–≤–∏—á–∫–∏:
1. –ü–∞—Ä—Å–∏–Ω–≥ HTML —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é BeautifulSoup
2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤ —Ç–∞ –º–µ—Ç–æ–¥—ñ–≤ –ø–æ—à—É–∫—É
3. –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ —Ç–∞–±–ª–∏—Ü—å, —Å–ø–∏—Å–∫—ñ–≤, —Ñ–æ—Ä–º
4. –û–±—Ä–æ–±–∫–∞ —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫

---

## üìö –ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞

### –ù–µ–æ–±—Ö—ñ–¥–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏:
```bash
pip install beautifulsoup4 lxml requests
```

### –§–∞–π–ª–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏:
- –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ –ø—Ä–æ–µ–∫—Ç–∏: `task1_weather.py`, `task2_currency.py`, `task3_rss.py`
- –ù–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥–æ–º (–±—É–¥—É—Ç—å —Å—Ç–≤–æ—Ä–µ–Ω—ñ)

---

## üìñ –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ (30 —Ö–≤–∏–ª–∏–Ω)

### 1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (15 —Ö–≤–∏–ª–∏–Ω)

#### –û—Å–Ω–æ–≤–∏ HTML
```html
<!DOCTYPE html>
<html lang="uk">
<head>
    <meta charset="UTF-8">
    <title>–ü—Ä–∏–∫–ª–∞–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏</title>
</head>
<body>
    <header>
        <h1>–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–æ—Ä—ñ–Ω–∫–∏</h1>
        <nav>
            <ul>
                <li><a href="/">–ì–æ–ª–æ–≤–Ω–∞</a></li>
                <li><a href="/news">–ù–æ–≤–∏–Ω–∏</a></li>
            </ul>
        </nav>
    </header>
    
    <main>
        <article class="news-article" id="article-1">
            <h2>–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—Ç—ñ</h2>
            <p class="author">–ê–≤—Ç–æ—Ä: –Ü–≤–∞–Ω –ü–µ—Ç—Ä–µ–Ω–∫–æ</p>
            <div class="content">
                <p>–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ...</p>
            </div>
        </article>
    </main>
    
    <footer>
        <p>&copy; 2025 –°–∞–π—Ç</p>
    </footer>
</body>
</html>
```

#### DOM (Document Object Model)
- –î–µ—Ä–µ–≤–æ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤
- –ë–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ —Ç–∞ –¥–æ—á—ñ—Ä–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
- –ê—Ç—Ä–∏–±—É—Ç–∏ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ (class, id, href, src)
- –ù–∞–≤—ñ–≥–∞—Ü—ñ—è –ø–æ DOM –¥–µ—Ä–µ–≤—É

### 2. CSS –°–µ–ª–µ–∫—Ç–æ—Ä–∏ (15 —Ö–≤–∏–ª–∏–Ω)

#### –¢–∏–ø–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤:

**1. –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –∑–∞ —Ç–µ–≥–æ–º:**
```css
p              /* –≤—Å—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ */
div            /* –≤—Å—ñ div –µ–ª–µ–º–µ–Ω—Ç–∏ */
a              /* –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è */
```

**2. –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –∑–∞ –∫–ª–∞—Å–æ–º:**
```css
.news-article  /* –µ–ª–µ–º–µ–Ω—Ç–∏ –∑ class="news-article" */
.author        /* –µ–ª–µ–º–µ–Ω—Ç–∏ –∑ class="author" */
```

**3. –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –∑–∞ ID:**
```css
#article-1     /* –µ–ª–µ–º–µ–Ω—Ç –∑ id="article-1" */
#header        /* –µ–ª–µ–º–µ–Ω—Ç –∑ id="header" */
```

**4. –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏:**
```css
div.news-article        /* div –∑ –∫–ª–∞—Å–æ–º news-article */
p.author                /* –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑ –∫–ª–∞—Å–æ–º author */
a.external              /* –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ –∫–ª–∞—Å–æ–º external */
```

**5. –Ü—î—Ä–∞—Ä—Ö—ñ—á–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏:**
```css
div p                   /* –≤—Å—ñ p –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ div */
article > h2            /* –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—ñ h2 –≤ article */
li + li                 /* li –ø—ñ—Å–ª—è —ñ–Ω—à–æ–≥–æ li */
```

**6. –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –∞—Ç—Ä–∏–±—É—Ç—ñ–≤:**
```css
a[href]                 /* –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑ –∞—Ç—Ä–∏–±—É—Ç–æ–º href */
img[src*="logo"]        /* img –¥–µ src –º—ñ—Å—Ç–∏—Ç—å "logo" */
input[type="text"]      /* input –∑ type="text" */
```

---

## üíª –ü—Ä–∞–∫—Ç–∏—á–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ (80 —Ö–≤–∏–ª–∏–Ω)

### –ß–∞—Å—Ç–∏–Ω–∞ 1: –û—Å–Ω–æ–≤–∏ BeautifulSoup (25 —Ö–≤–∏–ª–∏–Ω)

#### –ü—Ä–∏–∫–ª–∞–¥ 1: –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—Å—Ç–æ–≥–æ HTML

```python
from bs4 import BeautifulSoup
import requests

# HTML –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
html_doc = """
<html>
<head><title>–¢–µ—Å—Ç–æ–≤–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞</title></head>
<body>
    <h1>–ü—Ä–∏–≤—ñ—Ç, —Å–≤—ñ—Ç!</h1>
    <p class="intro">–¶–µ –ø–µ—Ä—à–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ.</p>
    <p class="content">–¶–µ –¥—Ä—É–≥–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑ <a href="https://example.com">–ø–æ—Å–∏–ª–∞–Ω–Ω—è–º</a>.</p>
    <ul id="news-list">
        <li class="news-item">–ù–æ–≤–∏–Ω–∞ 1</li>
        <li class="news-item">–ù–æ–≤–∏–Ω–∞ 2</li>
        <li class="news-item">–ù–æ–≤–∏–Ω–∞ 3</li>
    </ul>
</body>
</html>
"""

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–±'—î–∫—Ç—É BeautifulSoup
soup = BeautifulSoup(html_doc, 'html.parser')

# –†—ñ–∑–Ω—ñ –º–µ—Ç–æ–¥–∏ –ø–æ—à—É–∫—É
print("1. –ü–æ—à—É–∫ –∑–∞ —Ç–µ–≥–æ–º:")
h1 = soup.find('h1')
print(f"   {h1.text}")

print("\n2. –ü–æ—à—É–∫ –∑–∞ –∫–ª–∞—Å–æ–º:")
intro = soup.find('p', class_='intro')
print(f"   {intro.text}")

print("\n3. –ü–æ—à—É–∫ –∑–∞ ID:")
news_list = soup.find('ul', id='news-list')
print(f"   –ó–Ω–∞–π–¥–µ–Ω–æ: {news_list.name}")

print("\n4. –ü–æ—à—É–∫ –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤:")
all_p = soup.find_all('p')
for p in all_p:
    print(f"   - {p.text}")

print("\n5. –ü–æ—à—É–∫ –∑ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º:")
news_items = soup.select('li.news-item')
for item in news_items:
    print(f"   - {item.text}")

print("\n6. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∞—Ç—Ä–∏–±—É—Ç—ñ–≤:")
link = soup.find('a')
print(f"   –¢–µ–∫—Å—Ç: {link.text}")
print(f"   URL: {link.get('href')}")
```

#### –ü—Ä–∏–∫–ª–∞–¥ 2: –ù–∞–≤—ñ–≥–∞—Ü—ñ—è –ø–æ DOM –¥–µ—Ä–µ–≤—É

```python
from bs4 import BeautifulSoup

html = """
<div class="container">
    <h2>–ó–∞–≥–æ–ª–æ–≤–æ–∫</h2>
    <div class="content">
        <p>–ü–µ—Ä—à–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
        <p>–î—Ä—É–≥–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
        <ul>
            <li>–ü—É–Ω–∫—Ç 1</li>
            <li>–ü—É–Ω–∫—Ç 2</li>
        </ul>
    </div>
</div>
"""

soup = BeautifulSoup(html, 'html.parser')

# –ó–Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
container = soup.find('div', class_='container')

print("–î–æ—á—ñ—Ä–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏:")
for child in container.children:
    if child.name:  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—ñ –≤—É–∑–ª–∏
        print(f"  - {child.name}")

print("\n–í—Å—ñ –Ω–∞—â–∞–¥–∫–∏:")
for descendant in container.descendants:
    if descendant.name:
        print(f"  - {descendant.name}")

# –ù–∞–≤—ñ–≥–∞—Ü—ñ—è
content_div = soup.find('div', class_='content')
print("\n–ë–∞—Ç—å–∫—ñ–≤—Å—å–∫–∏–π –µ–ª–µ–º–µ–Ω—Ç:", content_div.parent.name)

first_p = content_div.find('p')
print("–ù–∞—Å—Ç—É–ø–Ω–∏–π –µ–ª–µ–º–µ–Ω—Ç:", first_p.find_next_sibling().text)
```

### –ß–∞—Å—Ç–∏–Ω–∞ 2: –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è –ø—Ä–æ–µ–∫—Ç—É Currency Converter (30 —Ö–≤–∏–ª–∏–Ω)

#### –ó–∞–≤–¥–∞–Ω–Ω—è: –î–æ–¥–∞—Ç–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –∫—É—Ä—Å—ñ–≤ –ù–ë–£

**–ú–µ—Ç–∞:** –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∫—É—Ä—Å–∏ –∑ API —Ç–∞ –æ—Ñ—ñ—Ü—ñ–π–Ω–æ–≥–æ —Å–∞–π—Ç—É –ù–ë–£

```python
"""
–†–æ–∑—à–∏—Ä–µ–Ω–Ω—è task2_currency.py
–î–æ–¥–∞–≤–∞–Ω–Ω—è –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É –∫—É—Ä—Å—ñ–≤ –ù–ë–£
"""

from bs4 import BeautifulSoup
import requests
from typing import Dict, List, Optional
from datetime import datetime


class NBUScraperMixin:
    """
    –ú—ñ–∫—Å—ñ–Ω –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É –∑ —Å–∞–π—Ç—É –ù–ë–£
    """
    
    NBU_URL = 'https://bank.gov.ua/ua/markets/exchangerates'
    
    def scrape_nbu_rates(self) -> Optional[Dict[str, float]]:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –∫—É—Ä—Å–∏ –≤–∞–ª—é—Ç –∑ —Å–∞–π—Ç—É –ù–ë–£ –º–µ—Ç–æ–¥–æ–º –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É
        
        Returns:
            Dict –∑ –∫—É—Ä—Å–∞–º–∏ –≤–∞–ª—é—Ç –∞–±–æ None
        """
        try:
            print("üåê –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –ù–ë–£...")
            response = requests.get(self.NBU_URL, timeout=10)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ó–Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—é –∑ –∫—É—Ä—Å–∞–º–∏
            # –ü—Ä–∏–º—ñ—Ç–∫–∞: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Å–∞–π—Ç—É
            table = soup.find('table', class_='currency-table')
            
            if not table:
                print("‚ùå –¢–∞–±–ª–∏—Ü—é –∫—É—Ä—Å—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return None
            
            rates = {}
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ä—è–¥–∫—ñ–≤ —Ç–∞–±–ª–∏—Ü—ñ
            rows = table.find_all('tr')[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 3:
                    currency_code = cols[0].text.strip()
                    rate_text = cols[2].text.strip()
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É –≤ —á–∏—Å–ª–æ
                    try:
                        rate = float(rate_text.replace(',', '.'))
                        rates[currency_code] = rate
                    except ValueError:
                        continue
            
            print(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(rates)} –∫—É—Ä—Å—ñ–≤ –∑ –ù–ë–£")
            return rates
            
        except requests.exceptions.RequestException as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            return None
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            return None
    
    def compare_with_nbu(self, currency: str = 'USD'):
        """
        –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∫—É—Ä—Å–∏ –∑ API —Ç–∞ –ù–ë–£
        
        Args:
            currency: –ö–æ–¥ –≤–∞–ª—é—Ç–∏
        """
        # –ö—É—Ä—Å –∑ API
        api_rates = self.get_rates('UAH')
        
        if not api_rates or currency not in api_rates:
            print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫—É—Ä—Å {currency} –∑ API")
            return
        
        api_rate = api_rates[currency]
        
        # –ö—É—Ä—Å –∑ –ù–ë–£
        nbu_rates = self.scrape_nbu_rates()
        
        if not nbu_rates or currency not in nbu_rates:
            print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫—É—Ä—Å {currency} –∑ –ù–ë–£")
            return
        
        nbu_rate = nbu_rates[currency]
        
        # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        difference = api_rate - nbu_rate
        difference_percent = (difference / nbu_rate) * 100
        
        print("\n" + "="*60)
        print(f"üìä –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ö–£–†–°–Ü–í {currency}/UAH")
        print("="*60)
        print(f"API –∫—É—Ä—Å:    {api_rate:.4f}")
        print(f"–ù–ë–£ –∫—É—Ä—Å:    {nbu_rate:.4f}")
        print(f"–†—ñ–∑–Ω–∏—Ü—è:     {difference:.4f} ({difference_percent:+.2f}%)")
        
        if abs(difference_percent) > 5:
            print("‚ö†Ô∏è  –ó–Ω–∞—á–Ω–∞ —Ä—ñ–∑–Ω–∏—Ü—è –≤ –∫—É—Ä—Å–∞—Ö!")
        else:
            print("‚úÖ –ö—É—Ä—Å–∏ –ø—Ä–∏–±–ª–∏–∑–Ω–æ –æ–¥–Ω–∞–∫–æ–≤—ñ")
        
        print("="*60)


# –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ —ñ—Å–Ω—É—é—á–∏–º –∫–ª–∞—Å–æ–º
class EnhancedCurrencyConverter(NBUScraperMixin, CurrencyConverter):
    """
    –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –≤–∞–ª—é—Ç –∑ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥–æ–º
    """
    pass


def demo_nbu_scraping():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É –ù–ë–£"""
    print("\n" + "="*70)
    print("–î–ï–ú–û: –í–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –∫—É—Ä—Å—ñ–≤ –ù–ë–£")
    print("="*70)
    
    converter = EnhancedCurrencyConverter()
    
    # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫—É—Ä—Å—ñ–≤
    for currency in ['USD', 'EUR', 'GBP']:
        converter.compare_with_nbu(currency)
        print()


if __name__ == '__main__':
    demo_nbu_scraping()
```

### –ß–∞—Å—Ç–∏–Ω–∞ 3: –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è RSS Aggregator (25 —Ö–≤–∏–ª–∏–Ω)

#### –ó–∞–≤–¥–∞–Ω–Ω—è: –î–æ–¥–∞—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤

**–ú–µ—Ç–∞:** –î–æ–ø–æ–≤–Ω–∏—Ç–∏ RSS –Ω–æ–≤–∏–Ω–∏ –ø—Ä—è–º–∏–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫

```python
"""
–†–æ–∑—à–∏—Ä–µ–Ω–Ω—è task3_rss.py
–î–æ–¥–∞–≤–∞–Ω–Ω—è –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
"""

from bs4 import BeautifulSoup
import requests
from typing import List, Dict, Optional
from datetime import datetime
import re


class WebScraperMixin:
    """
    –ú—ñ–∫—Å—ñ–Ω –¥–ª—è –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
    """
    
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
    SCRAPING_CONFIGS = {
        'threatpost': {
            'url': 'https://threatpost.com/',
            'article_selector': 'article.c-card',
            'title_selector': 'h2.c-card__title',
            'link_selector': 'a.c-card__link',
            'date_selector': 'time.c-card__time'
        },
        'bleeping_computer': {
            'url': 'https://www.bleepingcomputer.com/news/security/',
            'article_selector': 'div.bc_latest_news_text',
            'title_selector': 'h4',
            'link_selector': 'a',
            'date_selector': 'div.bc_news_date'
        }
    }
    
    def scrape_website(self, site_name: str) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–∏–Ω–Ω–æ–≥–æ —Å–∞–π—Ç—É
        
        Args:
            site_name: –ù–∞–∑–≤–∞ —Å–∞–π—Ç—É –∑ SCRAPING_CONFIGS
            
        Returns:
            List —Å—Ç–∞—Ç–µ–π
        """
        if site_name not in self.SCRAPING_CONFIGS:
            print(f"‚ùå –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è '{site_name}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return []
        
        config = self.SCRAPING_CONFIGS[site_name]
        
        try:
            print(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {site_name}...")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            response = requests.get(
                config['url'], 
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0'}
            )
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–Ω–≥
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ —Å—Ç–∞—Ç—Ç—ñ
            articles_html = soup.select(config['article_selector'])
            
            articles = []
            
            for article_html in articles_html[:10]:  # –û–±–º–µ–∂–∏—Ç–∏ 10 —Å—Ç–∞—Ç—Ç—è–º–∏
                try:
                    # –í–∏—Ç—è–≥—Ç–∏ –¥–∞–Ω—ñ
                    title_elem = article_html.select_one(config['title_selector'])
                    link_elem = article_html.select_one(config['link_selector'])
                    date_elem = article_html.select_one(config['date_selector'])
                    
                    if not title_elem or not link_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    
                    # –ü–æ–≤–Ω–∏–π URL
                    if link and not link.startswith('http'):
                        base_url = config['url'].rstrip('/')
                        link = base_url + link
                    
                    date = date_elem.get_text(strip=True) if date_elem else 'Unknown'
                    
                    article = {
                        'source': site_name.replace('_', ' ').title(),
                        'title': title,
                        'link': link,
                        'published': date,
                        'description': '',  # –ü–æ—Ç—Ä–µ–±—É—î –æ–∫—Ä–µ–º–æ–≥–æ –∑–∞–ø–∏—Ç—É
                        'scraped': True  # –ú—ñ—Ç–∫–∞, —â–æ –æ—Ç—Ä–∏–º–∞–Ω–æ —Å–∫—Ä–∞–ø—ñ–Ω–≥–æ–º
                    }
                    
                    articles.append(article)
                    
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É —Å—Ç–∞—Ç—Ç—ñ: {e}")
                    continue
            
            print(f"  ‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
            return articles
            
        except requests.exceptions.RequestException as e:
            print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            return []
        except Exception as e:
            print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            return []
    
    def fetch_article_content(self, url: str) -> Optional[str]:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ–≤–Ω–∏–π –≤–º—ñ—Å—Ç —Å—Ç–∞—Ç—Ç—ñ
        
        Args:
            url: URL —Å—Ç–∞—Ç—Ç—ñ
            
        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ –∞–±–æ None
        """
        try:
            response = requests.get(
                url, 
                timeout=10,
                headers={'User-Agent': 'Mozilla/5.0'}
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –°–ø—Ä–æ–±–∞ –∑–Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            # (–º–æ–∂–µ –ø–æ—Ç—Ä–µ–±—É–≤–∞—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–∞–π—Ç—É)
            content_selectors = [
                'article',
                'div.article-content',
                'div.post-content',
                'div.entry-content'
            ]
            
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    # –í–∏–¥–∞–ª–∏—Ç–∏ —Å–∫—Ä–∏–ø—Ç–∏ —Ç–∞ —Å—Ç–∏–ª—ñ
                    for tag in content(['script', 'style', 'nav', 'aside']):
                        tag.decompose()
                    
                    # –í–∏—Ç—è–≥—Ç–∏ —Ç–µ–∫—Å—Ç
                    text = content.get_text(separator='\n', strip=True)
                    return text
            
            return None
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É: {e}")
            return None
    
    def scrape_all_configured_sites(self):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å—ñ—Ö –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
        """
        print("\nüîÑ –í–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤...")
        
        for site_name in self.SCRAPING_CONFIGS.keys():
            articles = self.scrape_website(site_name)
            self.articles.extend(articles)
        
        print(f"‚úÖ –î–æ–¥–∞–Ω–æ {len(self.articles)} —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ —Å–∫—Ä–∞–ø—ñ–Ω–≥")


# –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ —ñ—Å–Ω—É—é—á–∏–º –∫–ª–∞—Å–æ–º
class EnhancedRSSAggregator(WebScraperMixin, RSSAggregator):
    """
    –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –∑ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥–æ–º
    """
    
    def fetch_all_sources(self):
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞–Ω—ñ –∑ RSS —Ç–∞ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É
        """
        # RSS
        self.fetch_all_feeds()
        
        # –í–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥
        self.scrape_all_configured_sites()


def demo_web_scraping():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É"""
    print("\n" + "="*70)
    print("–î–ï–ú–û: –í–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤")
    print("="*70)
    
    aggregator = EnhancedRSSAggregator()
    
    # –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–π –∑–±—ñ—Ä –¥–∞–Ω–∏—Ö
    aggregator.fetch_all_sources()
    
    # –ü–æ–∫–∞–∑–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å—å–æ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(aggregator.articles)}")
    
    # –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –∑–∞ –¥–∂–µ—Ä–µ–ª–∞–º–∏
    rss_articles = [a for a in aggregator.articles if not a.get('scraped')]
    scraped_articles = [a for a in aggregator.articles if a.get('scraped')]
    
    print(f"   –ó RSS: {len(rss_articles)}")
    print(f"   –ó—ñ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É: {len(scraped_articles)}")
    
    # –ü–æ–∫–∞–∑–∞—Ç–∏ —Ç–æ–ø
    aggregator.print_articles(aggregator.articles[:15])


if __name__ == '__main__':
    demo_web_scraping()
```

---

## üéì –°–∞–º–æ—Å—Ç—ñ–π–Ω–∞ —Ä–æ–±–æ—Ç–∞ (10 —Ö–≤–∏–ª–∏–Ω –Ω–∞ –∑–∞–Ω—è—Ç—Ç—è + –¥–æ–º–∞—à–Ω—î –∑–∞–≤–¥–∞–Ω–Ω—è)

### –ó–∞–≤–¥–∞–Ω–Ω—è –Ω–∞ –∑–∞–Ω—è—Ç—Ç—ñ:

1. **–°—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ—Å—Ç–∏–π —Å–∫—Ä–∞–ø–µ—Ä** (5 —Ö–≤)
   - –û–±—Ä–∞—Ç–∏ –±—É–¥—å-—è–∫–∏–π –Ω–æ–≤–∏–Ω–Ω–∏–π —Å–∞–π—Ç
   - –ù–∞–ø–∏—Å–∞—Ç–∏ –∫–æ–¥ –¥–ª—è –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ —Å—Ç–∞—Ç–µ–π
   - –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∏

2. **–Ü–Ω—Ç–µ–≥—Ä—É–≤–∞—Ç–∏ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏** (5 —Ö–≤)
   - –î–æ–¥–∞—Ç–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –¥–æ –æ–¥–Ω–æ–≥–æ –∑ –ø—Ä–æ–µ–∫—Ç—ñ–≤
   - –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª

### –î–æ–º–∞—à–Ω—î –∑–∞–≤–¥–∞–Ω–Ω—è:

#### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –†–æ–∑—à–∏—Ä–∏—Ç–∏ Weather Monitor (—Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å: ‚òÖ‚òÖ‚òÜ)
–î–æ–¥–∞—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ–≥–æ–¥–∏ –∑ –≤–µ–±-—Å–∞–π—Ç—ñ–≤ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, sinoptik.ua):
- –ü–∞—Ä—Å–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–∏–∂–¥–µ–Ω—å
- –ü–æ—Ä—ñ–≤–Ω—é–≤–∞—Ç–∏ –¥–∞–Ω—ñ –∑ API
- –°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –∑–≤—ñ—Ç

#### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –°—Ç–≤–æ—Ä–∏—Ç–∏ Price Monitor (—Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å: ‚òÖ‚òÖ‚òÖ)
–°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ü—ñ–Ω –Ω–∞ —Ç–æ–≤–∞—Ä–∏:
- –ü–∞—Ä—Å–∏—Ç–∏ —Ü—ñ–Ω–∏ –∑ 2-3 —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω—ñ–≤
- –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ —ñ—Å—Ç–æ—Ä—ñ—é —Ü—ñ–Ω
- –í—ñ–¥–ø—Ä–∞–≤–ª—è—Ç–∏ –∞–ª–µ—Ä—Ç–∏ –ø—Ä–∏ –∑–Ω–∏–∂–µ–Ω–Ω—ñ —Ü—ñ–Ω–∏
- –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è

#### –ó–∞–≤–¥–∞–Ω–Ω—è 3: Job Scraper (—Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å: ‚òÖ‚òÖ‚òÖ)
–°—Ç–≤–æ—Ä–∏—Ç–∏ —Å–∫—Ä–∞–ø–µ—Ä –≤–∞–∫–∞–Ω—Å—ñ–π:
- –ü–∞—Ä—Å–∏—Ç–∏ –≤–∞–∫–∞–Ω—Å—ñ—ó –∑ djinni.co –∞–±–æ work.ua
- –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
- –í–∏—Ç—è–≥—É–≤–∞—Ç–∏ –≤–∏–º–æ–≥–∏ –¥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤
- –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –∑–≤—ñ—Ç

---

## üìù –í–∞–∂–ª–∏–≤—ñ –ø—Ä–∏–º—ñ—Ç–∫–∏ —Ç–∞ best practices

### –ï—Ç–∏—á–Ω—ñ –∞—Å–ø–µ–∫—Ç–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É:

1. **Robots.txt**
   ```python
   import requests
   from urllib.parse import urljoin
   
   def check_robots_txt(base_url: str) -> str:
       """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ robots.txt"""
       robots_url = urljoin(base_url, '/robots.txt')
       try:
           response = requests.get(robots_url)
           if response.status_code == 200:
               return response.text
       except:
           pass
       return ""
   ```

2. **–ó–∞—Ç—Ä–∏–º–∫–∏ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏**
   ```python
   import time
   
   # –î–æ–¥–∞–≤–∞—Ç–∏ –∑–∞—Ç—Ä–∏–º–∫—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
   time.sleep(1)  # 1 —Å–µ–∫—É–Ω–¥–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
   ```

3. **User-Agent**
   ```python
   headers = {
       'User-Agent': 'Mozilla/5.0 (educational purpose)'
   }
   requests.get(url, headers=headers)
   ```

### –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫:

```python
from requests.exceptions import RequestException

try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'html.parser')
except RequestException as e:
    print(f"–ü–æ–º–∏–ª–∫–∞ –º–µ—Ä–µ–∂—ñ: {e}")
except Exception as e:
    print(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
```

---

## üîç –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ñ –ø–∏—Ç–∞–Ω–Ω—è

1. –©–æ —Ç–∞–∫–µ DOM —ñ —è–∫ –≤—ñ–Ω –ø–æ–≤'—è–∑–∞–Ω–∏–π –∑ HTML?
2. –£ —á–æ–º—É —Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ `.find()` —Ç–∞ `.find_all()` –≤ BeautifulSoup?
3. –Ø–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –≤ BeautifulSoup?
4. –Ø–∫—ñ –µ—Ç–∏—á–Ω—ñ –∞—Å–ø–µ–∫—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏ –ø—Ä–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É?
5. –Ø–∫ –æ–±—Ä–æ–±–ª—è—Ç–∏ –ø–æ–º–∏–ª–∫–∏ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥—É HTML?
6. –£ —á–æ–º—É —Ä—ñ–∑–Ω–∏—Ü—è –º—ñ–∂ –ø–∞—Ä—Å–µ—Ä–æ–º 'html.parser' —Ç–∞ 'lxml'?

---

## üìö –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ä–µ—Å—É—Ä—Å–∏

1. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è BeautifulSoup**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
2. **CSS Selectors Reference**: https://www.w3schools.com/cssref/css_selectors.asp
3. **Requests documentation**: https://requests.readthedocs.io/
4. **HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**: https://developer.mozilla.org/en-US/docs/Web/HTML

---

## ‚úÖ –ß–µ–∫–ª–∏—Å—Ç –¥–ª—è –≤–∏–∫–ª–∞–¥–∞—á–∞

- [ ] –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω—ñ HTML —Ñ–∞–π–ª–∏
- [ ] –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –≤–µ–±-—Å–∞–π—Ç—ñ–≤ –¥–ª—è –¥–µ–º–æ
- [ ] –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ —Ä–µ–∑–µ—Ä–≤–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ (–Ω–∞ –≤–∏–ø–∞–¥–æ–∫ –∑–º—ñ–Ω –Ω–∞ —Å–∞–π—Ç–∞—Ö)
- [ ] –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ BeautifulSoup —Ç–∞ lxml
- [ ] –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –∫–æ–¥ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –∑–∞–Ω—è—Ç—å
- [ ] –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó
- [ ] –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –º–∞—Ç–µ—Ä—ñ–∞–ª–∏ –ø—Ä–æ –µ—Ç–∏—á–Ω–∏–π —Å–∫—Ä–∞–ø—ñ–Ω–≥

---

## üéØ –ö—Ä–∏—Ç–µ—Ä—ñ—ó –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è

### –†–æ–±–æ—Ç–∞ –Ω–∞ –∑–∞–Ω—è—Ç—Ç—ñ (60%):
- –†–æ–∑—É–º—ñ–Ω–Ω—è CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤ (20%)
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è BeautifulSoup (20%)
- –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏ (20%)

### –î–æ–º–∞—à–Ω—î –∑–∞–≤–¥–∞–Ω–Ω—è (40%):
- –§—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å —Å–∫—Ä–∞–ø–µ—Ä–∞ (15%)
- –û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ (10%)
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥—É (10%)
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è —Ç–∞ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ (5%)

---

**–£—Å–ø—ñ—Ö—ñ–≤ —É –≤–∏–≤—á–µ–Ω–Ω—ñ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É! üöÄ**
