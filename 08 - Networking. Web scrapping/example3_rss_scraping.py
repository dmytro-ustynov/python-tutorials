"""
–ü—Ä–∞–∫—Ç–∏—á–Ω–µ –∑–∞–Ω—è—Ç—Ç—è 4-4: –û—Å–Ω–æ–≤–∏ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É
–ü—Ä–∏–∫–ª–∞–¥ 3: –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ RSS Aggregator

–†–æ–∑—à–∏—Ä–µ–Ω–Ω—è task3_rss.py –∑ –ø—Ä—è–º–∏–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
"""

from bs4 import BeautifulSoup
import requests
import feedparser
from datetime import datetime
from typing import List, Dict, Optional
import json
from pathlib import Path
import re


class RSSAggregator:
    """–ë–∞–∑–æ–≤–∏–π RSS –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä (–∑ task3_rss.py)"""
    
    DEFAULT_FEEDS = {
        'The Hacker News': 'https://feeds.feedburner.com/TheHackersNews',
        'Krebs on Security': 'https://krebsonsecurity.com/feed/',
    }
    
    def __init__(self, feeds_file: str = 'feeds.json'):
        self.feeds_file = Path(feeds_file)
        self.feeds = self._load_feeds()
        self.articles = []
    
    def _load_feeds(self) -> Dict[str, str]:
        if self.feeds_file.exists():
            with open(self.feeds_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return self.DEFAULT_FEEDS.copy()
    
    def fetch_feed(self, url: str) -> Optional[feedparser.FeedParserDict]:
        try:
            feed = feedparser.parse(url)
            return feed if not feed.bozo else None
        except:
            return None
    
    def fetch_all_feeds(self):
        self.articles = []
        print("\nüîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RSS –∫–∞–Ω–∞–ª—ñ–≤...")
        
        for name, url in self.feeds.items():
            print(f"  ‚Ä¢ {name}...", end=" ")
            feed = self.fetch_feed(url)
            
            if feed and feed.entries:
                for entry in feed.entries:
                    article = {
                        'source': name,
                        'title': entry.get('title', 'No title'),
                        'link': entry.get('link', ''),
                        'published': entry.get('published', 'Unknown'),
                        'summary': entry.get('summary', ''),
                        'type': 'rss'
                    }
                    self.articles.append(article)
                print(f"‚úÖ {len(feed.entries)} —Å—Ç–∞—Ç–µ–π")
            else:
                print("‚ùå")
    
    @staticmethod
    def _clean_html(text: str) -> str:
        clean = re.sub('<.*?>', '', text)
        clean = re.sub(r'\s+', ' ', clean)
        return clean.strip()


class WebScraperMixin:
    """–ú—ñ–∫—Å—ñ–Ω –¥–ª—è –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤"""
    
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
    SCRAPING_CONFIGS = {
        'bleeping_computer': {
            'name': 'Bleeping Computer',
            'url': 'https://www.bleepingcomputer.com/news/security/',
            'article_selector': 'article.bc_latest_news',
            'title_selector': 'h4 a',
            'link_selector': 'h4 a',
            'summary_selector': 'p.bc_latest_news_text',
            'date_selector': 'li.bc_news_date'
        },
        'demo_site': {
            'name': 'Demo Security News',
            'url': 'demo',  # –ë—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¥–µ–º–æ HTML
            'article_selector': 'article.news-item',
            'title_selector': 'h2.title',
            'link_selector': 'a.read-more',
            'summary_selector': 'p.summary',
            'date_selector': 'time.published'
        }
    }
    
    def scrape_website(self, site_key: str) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–∏–Ω–Ω–æ–≥–æ —Å–∞–π—Ç—É
        
        Args:
            site_key: –ö–ª—é—á —Å–∞–π—Ç—É –∑ SCRAPING_CONFIGS
            
        Returns:
            List —Å—Ç–∞—Ç–µ–π
        """
        if site_key not in self.SCRAPING_CONFIGS:
            print(f"‚ùå –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è '{site_key}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return []
        
        config = self.SCRAPING_CONFIGS[site_key]
        
        # –î–ª—è –¥–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–µ—Å—Ç–æ–≤–∏–π HTML
        if site_key == 'demo_site':
            return self._scrape_demo_site(config)
        
        try:
            print(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {config['name']}...")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            headers = {
                'User-Agent': 'Mozilla/5.0 (educational purpose)'
            }
            response = requests.get(
                config['url'],
                timeout=15,
                headers=headers
            )
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–Ω–≥
            return self._parse_articles(response.content, config)
            
        except requests.exceptions.RequestException as e:
            print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            print(f"  üí° –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–µ–º–æ –¥–∞–Ω—ñ –¥–ª—è {config['name']}")
            return self._get_demo_articles(config['name'])
        except Exception as e:
            print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            return []
    
    def _scrape_demo_site(self, config: Dict) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–æ–≥–æ HTML"""
        demo_html = """
        <html>
        <body>
            <article class="news-item">
                <h2 class="title">–ù–æ–≤–∞ –∫—Ä–∏—Ç–∏—á–Ω–∞ –≤—Ä–∞–∑–ª–∏–≤—ñ—Å—Ç—å —É Windows</h2>
                <time class="published">2025-11-04</time>
                <p class="summary">
                    Microsoft –≤–∏–ø—É—Å—Ç–∏–ª–∞ –µ–∫—Å—Ç—Ä–µ–Ω–∏–π –ø–∞—Ç—á –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–æ—ó –≤—Ä–∞–∑–ª–∏–≤–æ—Å—Ç—ñ,
                    —è–∫–∞ –¥–æ–∑–≤–æ–ª—è—î –≤—ñ–¥–¥–∞–ª–µ–Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫–æ–¥—É.
                </p>
                <a class="read-more" href="https://example.com/news/1">–ß–∏—Ç–∞—Ç–∏ –¥–∞–ª—ñ</a>
            </article>
            
            <article class="news-item">
                <h2 class="title">–ú–∞—Å—à—Ç–∞–±–Ω–∞ DDoS –∞—Ç–∞–∫–∞ –Ω–∞ –±–∞–Ω–∫–∏</h2>
                <time class="published">2025-11-03</time>
                <p class="summary">
                    –ö—ñ–ª—å–∫–∞ –≤–µ–ª–∏–∫–∏—Ö –±–∞–Ω–∫—ñ–≤ –∑–∞–∑–Ω–∞–ª–∏ DDoS –∞—Ç–∞–∫–∏, —â–æ –ø—Ä–∏–∑–≤–µ–ª–æ –¥–æ
                    —Ç–∏–º—á–∞—Å–æ–≤–æ–≥–æ –ø—Ä–∏–ø–∏–Ω–µ–Ω–Ω—è –æ–Ω–ª–∞–π–Ω-—Å–µ—Ä–≤—ñ—Å—ñ–≤.
                </p>
                <a class="read-more" href="https://example.com/news/2">–ß–∏—Ç–∞—Ç–∏ –¥–∞–ª—ñ</a>
            </article>
            
            <article class="news-item">
                <h2 class="title">–í–∏—è–≤–ª–µ–Ω–æ –Ω–æ–≤—É ransomware –≥—Ä—É–ø—É</h2>
                <time class="published">2025-11-02</time>
                <p class="summary">
                    –î–æ—Å–ª—ñ–¥–Ω–∏–∫–∏ –±–µ–∑–ø–µ–∫–∏ –≤–∏—è–≤–∏–ª–∏ –Ω–æ–≤—É –≥—Ä—É–ø—É –∫—ñ–±–µ—Ä–∑–ª–æ—á–∏–Ω—Ü—ñ–≤,
                    —â–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è –Ω–∞ ransomware –∞—Ç–∞–∫–∞—Ö.
                </p>
                <a class="read-more" href="https://example.com/news/3">–ß–∏—Ç–∞—Ç–∏ –¥–∞–ª—ñ</a>
            </article>
            
            <article class="news-item">
                <h2 class="title">–û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–æ–ª—ñ—Ç–∏–∫–∏ –∫—ñ–±–µ—Ä–±–µ–∑–ø–µ–∫–∏ –Ñ–°</h2>
                <time class="published">2025-11-01</time>
                <p class="summary">
                    –Ñ–≤—Ä–æ–ø–µ–π—Å—å–∫–∏–π –°–æ—é–∑ –æ–≥–æ–ª–æ—Å–∏–≤ –ø—Ä–æ –Ω–æ–≤—ñ –≤–∏–º–æ–≥–∏ –¥–æ –∫—ñ–±–µ—Ä–±–µ–∑–ø–µ–∫–∏
                    –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–æ—ó —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏.
                </p>
                <a class="read-more" href="https://example.com/news/4">–ß–∏—Ç–∞—Ç–∏ –¥–∞–ª—ñ</a>
            </article>
            
            <article class="news-item">
                <h2 class="title">–ü—ñ–¥—Å—É–º–∫–∏ —Ä–æ–∫—É: –≥–æ–ª–æ–≤–Ω—ñ –∫—ñ–±–µ—Ä—ñ–Ω—Ü–∏–¥–µ–Ω—Ç–∏</h2>
                <time class="published">2025-10-31</time>
                <p class="summary">
                    –û–≥–ª—è–¥ –Ω–∞–π–∑–Ω–∞—á–Ω—ñ—à–∏—Ö –∫—ñ–±–µ—Ä—ñ–Ω—Ü–∏–¥–µ–Ω—Ç—ñ–≤ —Ç–∞ –∞—Ç–∞–∫ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ä—ñ–∫.
                </p>
                <a class="read-more" href="https://example.com/news/5">–ß–∏—Ç–∞—Ç–∏ –¥–∞–ª—ñ</a>
            </article>
        </body>
        </html>
        """
        
        print(f"üß™ –ü–∞—Ä—Å–∏–Ω–≥ {config['name']} (–¥–µ–º–æ —Ä–µ–∂–∏–º)...")
        return self._parse_articles(demo_html, config)
    
    def _parse_articles(self, html_content, config: Dict) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π –∑ HTML"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ —Å—Ç–∞—Ç—Ç—ñ
        articles_html = soup.select(config['article_selector'])
        
        if not articles_html:
            print(f"  ‚ö†Ô∏è  –°—Ç–∞—Ç—Ç—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {config['article_selector']}")
            return []
        
        articles = []
        
        for article_html in articles_html[:10]:  # –ú–∞–∫—Å–∏–º—É–º 10
            try:
                # –í–∏—Ç—è–≥—Ç–∏ –¥–∞–Ω—ñ
                title_elem = article_html.select_one(config['title_selector'])
                link_elem = article_html.select_one(config['link_selector'])
                summary_elem = article_html.select_one(config['summary_selector'])
                date_elem = article_html.select_one(config['date_selector'])
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = link_elem.get('href', '') if link_elem else ''
                summary = summary_elem.get_text(strip=True) if summary_elem else ''
                date = date_elem.get_text(strip=True) if date_elem else 'Unknown'
                
                # –ü–æ–≤–Ω–∏–π URL
                if link and not link.startswith('http'):
                    base_url = config['url'].rstrip('/')
                    if base_url != 'demo':
                        # –í–∏—Ç—è–≥—Ç–∏ base URL
                        from urllib.parse import urlparse
                        parsed = urlparse(config['url'])
                        base_url = f"{parsed.scheme}://{parsed.netloc}"
                        link = base_url + link
                
                article = {
                    'source': config['name'],
                    'title': title,
                    'link': link,
                    'published': date,
                    'summary': summary,
                    'type': 'scraped'
                }
                
                articles.append(article)
                
            except Exception as e:
                print(f"  ‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É —Å—Ç–∞—Ç—Ç—ñ: {e}")
                continue
        
        print(f"  ‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        return articles
    
    def _get_demo_articles(self, source_name: str) -> List[Dict]:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –¥–µ–º–æ —Å—Ç–∞—Ç—Ç—ñ (—è–∫—â–æ —Å–ø—Ä–∞–≤–∂–Ω—ñ–π —Å–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π)"""
        return [
            {
                'source': source_name,
                'title': '–î–µ–º–æ —Å—Ç–∞—Ç—Ç—è 1: –í—Ä–∞–∑–ª–∏–≤—ñ—Å—Ç—å –Ω—É–ª—å–æ–≤–æ–≥–æ –¥–Ω—è',
                'link': 'https://example.com/demo1',
                'published': '2025-11-04',
                'summary': '–í–∏—è–≤–ª–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω—É –≤—Ä–∞–∑–ª–∏–≤—ñ—Å—Ç—å –Ω—É–ª—å–æ–≤–æ–≥–æ –¥–Ω—è...',
                'type': 'scraped'
            },
            {
                'source': source_name,
                'title': '–î–µ–º–æ —Å—Ç–∞—Ç—Ç—è 2: –ö—ñ–±–µ—Ä–∞—Ç–∞–∫–∞ –Ω–∞ –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤–æ',
                'link': 'https://example.com/demo2',
                'published': '2025-11-03',
                'summary': '–í–µ–ª–∏–∫–æ–º–∞—Å—à—Ç–∞–±–Ω–∞ –∞—Ç–∞–∫–∞ –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—É —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É...',
                'type': 'scraped'
            }
        ]
    
    def scrape_all_configured_sites(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å—ñ—Ö –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤"""
        print("\nüîÑ –í–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤...")
        
        scraped_count = 0
        
        for site_key in self.SCRAPING_CONFIGS.keys():
            articles = self.scrape_website(site_key)
            self.articles.extend(articles)
            scraped_count += len(articles)
        
        print(f"‚úÖ –î–æ–¥–∞–Ω–æ {scraped_count} —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ —Å–∫—Ä–∞–ø—ñ–Ω–≥")
    
    def fetch_article_full_content(self, url: str) -> Optional[str]:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ–≤–Ω–∏–π –≤–º—ñ—Å—Ç —Å—Ç–∞—Ç—Ç—ñ
        
        Args:
            url: URL —Å—Ç–∞—Ç—Ç—ñ
            
        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ
        """
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, timeout=10, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –°–ø—Ä–æ–±–∞ –∑–Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_selectors = [
                'article',
                'div.article-content',
                'div.post-content',
                'div.entry-content',
                'main'
            ]
            
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    # –í–∏–¥–∞–ª–∏—Ç–∏ –∑–∞–π–≤–µ
                    for tag in content(['script', 'style', 'nav', 'aside', 'ad']):
                        tag.decompose()
                    
                    # –í–∏—Ç—è–≥—Ç–∏ —Ç–µ–∫—Å—Ç
                    text = content.get_text(separator='\n', strip=True)
                    return text
            
            return None
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É: {e}")
            return None


class EnhancedRSSAggregator(WebScraperMixin, RSSAggregator):
    """–†–æ–∑—à–∏—Ä–µ–Ω–∏–π RSS –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –∑ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥–æ–º"""
    
    def fetch_all_sources(self):
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞–Ω—ñ –∑ RSS —Ç–∞ –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É"""
        # RSS –∫–∞–Ω–∞–ª–∏
        self.fetch_all_feeds()
        
        # –í–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥
        self.scrape_all_configured_sites()
    
    def print_articles_summary(self):
        """–í–∏–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å—Ç–∞—Ç–µ–π"""
        if not self.articles:
            print("üì∞ –°—Ç–∞—Ç–µ–π –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return
        
        print("\n" + "="*70)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–Ü–ë–†–ê–ù–ò–• –°–¢–ê–¢–ï–ô")
        print("="*70)
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total = len(self.articles)
        rss_articles = [a for a in self.articles if a.get('type') == 'rss']
        scraped_articles = [a for a in self.articles if a.get('type') == 'scraped']
        
        print(f"\n–í—Å—å–æ–≥–æ —Å—Ç–∞—Ç–µ–π: {total}")
        print(f"  ‚Ä¢ –ó RSS –∫–∞–Ω–∞–ª—ñ–≤: {len(rss_articles)}")
        print(f"  ‚Ä¢ –ó—ñ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É: {len(scraped_articles)}")
        
        # –ó–∞ –¥–∂–µ—Ä–µ–ª–∞–º–∏
        print("\nüìç –ó–∞ –¥–∂–µ—Ä–µ–ª–∞–º–∏:")
        sources = {}
        for article in self.articles:
            source = article['source']
            sources[source] = sources.get(source, 0) + 1
        
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            print(f"  ‚Ä¢ {source}: {count} —Å—Ç–∞—Ç–µ–π")
        
        print("="*70)
    
    def print_articles(self, articles: List[Dict] = None, limit: int = None):
        """–í–∏–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π"""
        articles_to_print = articles if articles else self.articles
        
        if not articles_to_print:
            print("üì∞ –°—Ç–∞—Ç–µ–π –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return
        
        display_articles = articles_to_print[:limit] if limit else articles_to_print
        
        print("\n" + "="*70)
        print(f"üì∞ –°–¢–ê–¢–¢–Ü")
        print("="*70)
        
        for i, article in enumerate(display_articles, 1):
            type_icon = "üì°" if article.get('type') == 'rss' else "üåê"
            
            print(f"\n{i}. {type_icon} [{article['source']}] {article['title']}")
            print(f"   üìÖ {article['published']}")
            if article['link']:
                print(f"   üîó {article['link']}")
            
            # –°–∫–æ—Ä–æ—á–µ–Ω–∏–π summary
            summary = article.get('summary', '')
            if summary:
                if len(summary) > 200:
                    summary = summary[:200] + "..."
                print(f"   üí¨ {summary}")
        
        if limit and len(articles_to_print) > limit:
            print(f"\n... —Ç–∞ —â–µ {len(articles_to_print) - limit} —Å—Ç–∞—Ç–µ–π")
        
        print("="*70)


def demo_basic_scraping():
    """–î–µ–º–æ –±–∞–∑–æ–≤–æ–≥–æ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É"""
    print("\n" + "="*70)
    print("–î–ï–ú–û 1: –ë–∞–∑–æ–≤–∏–π –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥ –Ω–æ–≤–∏–Ω–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤")
    print("="*70)
    
    aggregator = EnhancedRSSAggregator()
    
    # –¢—ñ–ª—å–∫–∏ —Å–∫—Ä–∞–ø—ñ–Ω–≥
    aggregator.scrape_all_configured_sites()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    aggregator.print_articles_summary()
    
    # –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞—Ç—Ç—ñ
    aggregator.print_articles(limit=5)


def demo_combined_sources():
    """–î–µ–º–æ –∫–æ–º–±—ñ–Ω–æ–≤–∞–Ω–æ–≥–æ –∑–±–æ—Ä—É –¥–∞–Ω–∏—Ö"""
    print("\n" + "="*70)
    print("–î–ï–ú–û 2: –ö–æ–º–±—ñ–Ω–æ–≤–∞–Ω–∏–π –∑–±—ñ—Ä (RSS + –°–∫—Ä–∞–ø—ñ–Ω–≥)")
    print("="*70)
    
    aggregator = EnhancedRSSAggregator()
    
    # RSS + –°–∫—Ä–∞–ø—ñ–Ω–≥
    aggregator.fetch_all_sources()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    aggregator.print_articles_summary()
    
    # –¢–æ–ø —Å—Ç–∞—Ç–µ–π
    print("\nüîù –¢–û–ü-10 –û–°–¢–ê–ù–ù–Ü–• –°–¢–ê–¢–ï–ô:")
    aggregator.print_articles(limit=10)


def demo_article_content():
    """–î–µ–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
    print("\n" + "="*70)
    print("–î–ï–ú–û 3: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–∞—Ç—Ç—ñ")
    print("="*70)
    
    aggregator = EnhancedRSSAggregator()
    
    # –û—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–∞—Ç—Ç—ñ
    aggregator.scrape_all_configured_sites()
    
    if aggregator.articles:
        # –í–∑—è—Ç–∏ –ø–µ—Ä—à—É —Å—Ç–∞—Ç—Ç—é
        article = aggregator.articles[0]
        
        print(f"\nüìÑ –°—Ç–∞—Ç—Ç—è: {article['title']}")
        print(f"üîó URL: {article['link']}")
        
        print("\n‚è≥ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É...")
        
        # –î–ª—è –¥–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–∞–≥–ª—É—à–∫—É
        print("\nüìù –ü–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç:")
        print("-" * 70)
        print("(–î–ª—è –¥–µ–º–æ –ø–æ–∫–∞–∑–∞–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç)")
        print(article.get('summary', '–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π'))
        print("-" * 70)
        
        print("\nüí° –£ —Ä–µ–∞–ª—å–Ω–æ–º—É –∑–∞—Å—Ç–æ—Å—É–Ω–∫—É —Ç—É—Ç –±—É–¥–µ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ,")
        print("   –≤–∏—Ç—è–≥–Ω—É—Ç–∏–π –º–µ—Ç–æ–¥–æ–º fetch_article_full_content()")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("\n" + "="*70)
    print("üì∞ –†–û–ó–®–ò–†–ï–ù–ò–ô RSS –ê–ì–†–ï–ì–ê–¢–û–† –ó –í–ï–ë-–°–ö–†–ê–ü–Ü–ù–ì–û–ú")
    print("="*70)
    print("\n–Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –≤–µ–±-—Å–∫—Ä–∞–ø—ñ–Ω–≥—É –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–º –ø—Ä–æ–µ–∫—Ç–æ–º RSS Aggregator")
    
    try:
        # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
        demo_basic_scraping()
        demo_combined_sources()
        demo_article_content()
        
        print("\n" + "="*70)
        print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print("="*70)
        print("\nüí° –©–æ –±—É–ª–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–æ–≤–∞–Ω–æ:")
        print("   ‚úì –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π –∑ –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–æ–∫")
        print("   ‚úì –ö–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è RSS —Ç–∞ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É")
        print("   ‚úì –°—Ç—Ä—É–∫—Ç—É—Ä—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª")
        print("   ‚úì –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–∞—Ç–µ–π")
        
    except KeyboardInterrupt:
        print("\n\n‚è∏Ô∏è  –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—é –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
    except Exception as e:
        print(f"\n\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫
    try:
        from bs4 import BeautifulSoup
        import requests
        import feedparser
    except ImportError:
        print("‚ùå –ü–æ–º–∏–ª–∫–∞: –ù–µ–æ–±—Ö—ñ–¥–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ñ")
        print("–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —ó—Ö –∫–æ–º–∞–Ω–¥–æ—é:")
        print("pip install beautifulsoup4 requests feedparser")
        exit(1)
    
    main()
